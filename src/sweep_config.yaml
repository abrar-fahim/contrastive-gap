program: src/wandb_main.py
name: ICLR run, 512D DEFAULT
method: grid
metric:
  goal: minimize
  name: train_intermodality_loss
parameters:
  temperature:
    values: [0.01]
  learnable_temperature:
    values: [False]
  clip_projection_dim: 
    values: [512] # 512
  batch_size: 
    values: [64]
  vision_model: 
    values: [VIT] # RN50 or VIT or VIT16
  use_scheduler: 
    values: [no] # COSINE or EXP
  schedule_every:
    values: [400] # num steps NOT epochs
  n_warmup_steps:
    values: [10000]
  weight_decay:
    values: [0.1]
  train_from_scratch:
    values: [False]
  continue_from_checkpoint: 
    values: [False]
  train_from_pretrained: 
    values: [True]
  finetune_clip_backbone: 
    values: [True]
  finetune_multi_layer_projection: 
    values: [False]


  # LOSS STUFF
  intra_modality_loss: 
    values: [False]
  remove_contrastive_loss: 
    values: [False]
  cyclip_loss: 
    values: [False]
  simclr_loss: 
    values: [False]

  # ACTUALLY USING
  uniformity_loss: 
    values: [False]
  alignment_loss: 
    values: [False]
  cross_uniformity_loss: 
    values: [False]
  # weight_decay: min: 0.2 max: 0.6



  lr: 
    values: [1e-6] # 5e-4 from CyClip paper
  n_epochs: 
    values: [9]
  num_workers: 
    values: [8]
  zero_shot_acc_num_workers: 
    values: [4]

  # DATASET STUFF
  # dataset: values: [ClipDatasets.CONCEPTUAL_CAPTIONS.value]
  dataset: 
    values: [mscoco] # conceptual_captions, mscoco
  validation_dataset_size: 
    values: [512]
  validation_batch_size: 
    values: [512]
  use_small_trainloader: 
    values: [False] 
  cifar10_acc: 
    values: [False] 
  use_train_as_val: 
    values: [False] # SET

  save_encoder_hidden_states: 
    values: [False]
  n_embeds_to_save: 
    values: [512]

  seed: 
    values: [2, 24, 42]
  # others
  grad_cache:
    values: [False]
  grad_cache_multiplier: 
    values: [16]
  max_steps:
    values: [-1]
  i_t_loss_weight:
    values: [0.5]
  t_i_loss_weight:
    values: [0.5]
  encoder1_modality:
    values: [image]
  encoder2_modality:
    values: [text]
  same_encoder:
    values: [False]
  one_encoder:
    values: [False]
  common_projection_layer:
    values: [False]
  W_layer_gap: 
    values: [-1]
  shared_transformer_layers: 
    values: [False]
  same_inputs:
    values: [False]
  second_caption_offset:
    values: [False]
  mismatched_pairs:
    values: [False]
  selected_clip_model:
    values: [clip_finetuned_temp]

  # loss
  rsa_loss:
    values: [False]
  intra_modality_loss:
    values: [False]
  intra_modality_temperature:
    values: [0.01]
  pearson_loss:
    values: [False]
  scaled_denominator:
    values: [False]
  svd_loss:
    values: [False]
  uniform_cyclic_loss:
    values: [False]
  cyclic_direction_loss:
    values: [False]
  cosine_uniformity_loss:
    values: [False]
  cosine_align_loss:
    values: [False]

  # train 
  train_only_one_batch:
    values: [False]
  use_cached_val_batch:
    values: [False]
  delete_val_batch_first:
    values: [False]

  small_train_loader_batch_size:
    values: [6]

  cifar_batch_size:
    values: [128]
  

  openai_clip_model:
    values: [ViT-B/32]
  hf_clip_model:
    values: [openai/clip-vit-base-patch32]
  visualize_embeddings:
    values: [False]
  save_losses:
    values: [False]
  csv_path:
    values: [stats/]
  loss_file_name_template:
    values: [Ttemp_loss_seed_trainmode_captionencoder_dim_val_bsize_dataset_vmodel_pretrained_POST_PAPER]
  show_incorrect_images:
    values: [False]
  save_every:
    values: [200]
  do_checkpointing:
    values: [True]
  

  
  
  

  

  
  

